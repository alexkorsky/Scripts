{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability Exercise\n",
    "\n",
    "Welcome! Below you will implement two metrics for evaluating the readability of documents:\n",
    "\n",
    "1. Flesch–Kincaid readability Grade Index \n",
    "2. Gunning Fog Index\n",
    "\n",
    "The solutions are in [readability_solutions.py](./readability_solutions.py). You can also click the jupyter icon to see all the files in the folder.\n",
    "\n",
    "To load all the functions in the solutions, simply include `from solutions import *`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization\n",
    "\n",
    "Let's read-in our text files. We have three different texts files to play with: \n",
    "\n",
    "1. `physics.txt`: taken from a technical wikipedia article about a theoretical physics idea called [Supersymmetry](https://en.wikipedia.org/wiki/Supersymmetry)\n",
    "\n",
    "2. `APPL_10k_2017.txt`: the 2017 10-K Item IA for APPLE INC, taken from the EDGAR website\n",
    "\n",
    "3. `alice.txt`: Excerpts from \"Alice in Wonderland\", the novel is in the public domain and freely available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 10-Ks item 1A for CIK = 0001065088 ...\n",
      "writing EBAY_10k_2017.txt...\n",
      "writing EBAY_10k_2016.txt...\n",
      "writing EBAY_10k_2015.txt...\n",
      "writing EBAY_10k_2014.txt...\n",
      "writing EBAY_10k_2013.txt...\n",
      "downloading 10-Ks item 1A for CIK = 0000320193 ...\n",
      "writing AAPL_10k_2017.txt...\n",
      "writing AAPL_10k_2016.txt...\n",
      "writing AAPL_10k_2015.txt...\n",
      "writing AAPL_10k_2014.txt...\n",
      "writing AAPL_10k_2013.txt...\n",
      "downloading 10-Ks item 1A for CIK = 0001310067 ...\n",
      "writing SHLDQ_10k_2017.txt...\n",
      "writing SHLDQ_10k_2016.txt...\n",
      "writing SHLDQ_10k_2015.txt...\n",
      "writing SHLDQ_10k_2014.txt...\n",
      "writing SHLDQ_10k_2013.txt...\n"
     ]
    }
   ],
   "source": [
    "# download some excerpts from 10-K files\n",
    "\n",
    "from download10k import get_files\n",
    "\n",
    "CIK = {'ebay': '0001065088', 'apple':'0000320193', 'sears': '0001310067'}\n",
    "get_files(CIK['ebay'], 'EBAY')\n",
    "get_files(CIK['apple'], 'AAPL')\n",
    "get_files(CIK['sears'], 'SHLDQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences separated by ; are better viewed as multiple sentences\n",
    "# join combines all the newlines in the file\n",
    "\n",
    "f = open(\"physics.txt\", \"r\")\n",
    "text_phy=''.join(f).replace(';','.')\n",
    "\n",
    "f = open(\"alice.txt\", \"r\")\n",
    "text_alice=''.join(f).replace(';','.')\n",
    "\n",
    "f = open(\"AAPL_10k_2017.txt\", \"r\")\n",
    "text_10k=''.join(f).replace(';','.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following discussion of risk factors contains forward-looking statements. These risk factors may be important to understanding other statements in this Form 10-K. The following information should be read in conjunction with Part II, Item 7, “Management’s Discussion and Analysis of Financial Condition and Results of Operations” and the consolidated financial statements and related notes in Part II, Item 8, “Financial Statements and Supplementary Data” of this Form 10-K. The business, financia...\n",
      "\n",
      "In particle physics, supersymmetry (SUSY) is a principle that proposes a relationship between two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin. A type of spacetime symmetry, supersymmetry is a possible candidate for undiscovered particle physics, and seen as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to...\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice, “without pictures or conversations?”. So she was considering, in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of ge...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check out some of the texts\n",
    "print(text_10k[:500]+\"...\\n\")\n",
    "print(text_phy[:500]+\"...\\n\")\n",
    "print(text_alice[:500]+\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing\n",
    "Here, we need to define functions that can split our texts into sentences, and split our sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# tokenize and clean the text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from syllable_count import syllable_count\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenizer that selects out non letter and non symbol (i.e. all alphabets)\n",
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    return [ w for w in word_tokenizer.tokenize(sent) if w.isalpha() ]\n",
    "\n",
    "# for the sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# you can tokenize sentences by calling\n",
    "# sent_tokenize(document)\n",
    "\n",
    "# and tokenize words by calling\n",
    "# word_tokenize(sentence)\n",
    "\n",
    "# syllable_count counts the number of syllables in a word\n",
    "# it's included in syllable_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement three functions\n",
    "\n",
    "1. `sentence_count`: a simple function that returns the number of sentences in a document\n",
    "\n",
    "2. `word_count`: a simple function that returns the number of words in a sentence\n",
    "\n",
    "3. `hard_word_count`: a simple function that returns the number of words with more than 3 syllables, while removing suffixes like \"-ed\", and \"-ing\". This can be done by lemmatizing the word, i.e. `wnl.lemmatize(word, pos='v')`\n",
    "\n",
    "the function `word_tokenize(sentence)` will be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_count(text):\n",
    "    return len(sent_tokenizer.tokenize(text))\n",
    "\n",
    "def word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent)])\n",
    "\n",
    "def hard_word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent) \\\n",
    "                if syllable_count(wnl.lemmatize(w, pos='v'))>=3 ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Readability Grade-Levels\n",
    "\n",
    "Here, you will implement the two readability indices (grade levels). They are defined by\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Flesch–Kincaid Grade} \n",
    "= 0.39 \\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \\\\\n",
    "+11.8\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of hard words}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "-15.59\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Gunning-Fog Grade} \n",
    "=\\; &0.4 \\bigg[ \n",
    "\\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \n",
    "+100\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of syllables}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "To count syllables, we've added a syllable_count function you can access via \n",
    "\n",
    "```\n",
    "from syllable_count import syllable_count\n",
    "syllable_count(\"syllable\")\n",
    "```\n",
    "\n",
    "Below, implement the function `flesch_index` and `fog_index` that computes the readability grade level for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flesch_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_syllables = np.sum([ np.sum([ syllable_count(w) for w in word_tokenize(s) ]) \\\n",
    "                              for s in sentences ])\n",
    "    \n",
    "    return 0.39*(total_words/total_sentences) + \\\n",
    "            11.8*(total_syllables/total_words) - 15.59\n",
    "\n",
    "def fog_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_hard_words = np.sum([ hard_word_count(s) for s in sentences ])\n",
    "    \n",
    "    return 0.4*((total_words/total_sentences) + \\\n",
    "            100.0*(total_hard_words/total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Results\n",
    "\n",
    "Now that you've coded up the exercises, compute the grade levels for each of the texts given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.78094652406 9.73654188948\n",
      "16.3171712123 19.3225332001\n",
      "18.2108288106 21.5614490682\n"
     ]
    }
   ],
   "source": [
    "# to test the solutions\n",
    "# uncommon next line\n",
    "# from readability_solutions import *\n",
    "\n",
    "print(flesch_index(text_alice),fog_index(text_alice))\n",
    "print(flesch_index(text_phy),fog_index(text_phy))\n",
    "print(flesch_index(text_10k),fog_index(text_10k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect a grade level around 7-10 for `alice.txt`, and around 16-19 for `physics.txt`, and 18+ for financial documents! \n",
    "\n",
    "It turns out 10-Ks are really *hard* to read legal documents!\n",
    "Now, let's compute the readability for all the 10-Ks we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL_10k_2013.txt 18.1336596757 21.4219541786\n",
      "AAPL_10k_2014.txt 18.1536894665 21.533048686\n",
      "AAPL_10k_2015.txt 18.2144706379 21.6060051245\n",
      "AAPL_10k_2016.txt 18.2620196893 21.6361424013\n",
      "AAPL_10k_2017.txt 18.2108288106 21.5614490682\n",
      "EBAY_10k_2013.txt 17.2088261149 19.4673717189\n",
      "EBAY_10k_2014.txt 17.522305957 19.844332095\n",
      "EBAY_10k_2015.txt 17.1741438469 19.5172704435\n",
      "EBAY_10k_2016.txt 16.8119978036 19.2121925858\n",
      "EBAY_10k_2017.txt 16.988036714 19.3980211714\n",
      "SHLDQ_10k_2013.txt 16.8126305116 19.2154420317\n",
      "SHLDQ_10k_2014.txt 17.1138126995 19.5253765922\n",
      "SHLDQ_10k_2015.txt 18.304118527 21.0016011567\n",
      "SHLDQ_10k_2016.txt 18.7321020854 21.4781606764\n",
      "SHLDQ_10k_2017.txt 17.755571973 20.6452057848\n"
     ]
    }
   ],
   "source": [
    "filelist_10k=!ls *10k*txt\n",
    "\n",
    "\n",
    "flesch = []\n",
    "fog = []\n",
    "\n",
    "for file in filelist_10k:\n",
    "    with open(file, 'r') as f:\n",
    "        text=''.join(f).replace(';','.')\n",
    "        flesch.append(flesch_index(text))\n",
    "        fog.append(fog_index(text))\n",
    "        print(file, flesch[-1],fog[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superficially, and according to our readability metrics, reading 10-Ks is harder than reading articles on theoretical physics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise:\n",
    "How are the two readability grade-levels correlated? Compute the covariance matrix of the two readability indices we have on all the 10K documents, and make a scatter plot of Flesch index vs Fog index. Also perform a least-squared fit to the result and plot it as well.\n",
    "\n",
    "(change bottom cell to code and remove html tags for solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">\n",
    "\n",
    "#solution\n",
    "cov = np.cov(flesch,fog)\n",
    "print(cov)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(flesch,fog) \n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(flesch, fog)\n",
    "\n",
    "x=np.linspace(16.5,19,101)\n",
    "y=slope*x+intercept\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.xlabel(\"Flesch Index\")\n",
    "plt.ylabel(\"Fog Index\")\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
